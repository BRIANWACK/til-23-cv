{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "%cd .\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration & Wrangling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install fiftyone imagehash\n",
    "%pip uninstall fiftyone-db -y\n",
    "%pip install fiftyone-db-ubuntu2204 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.utils.yolo as fouy\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "from tqdm import tqdm\n",
    "\n",
    "from til_23_cv import ReIDEncoder, cos_sim, thres_strategy_A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"til23plush\"\n",
    "dataset_dir = \"data/til23plush\"\n",
    "splits = \"train\", \"val\", \"test\"\n",
    "\n",
    "# NOTE: Uncomment to recache dataset\n",
    "# fo.delete_dataset(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:        til23plush\n",
      "Media type:  image\n",
      "Num samples: 8064\n",
      "Persistent:  True\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:           fiftyone.core.fields.ObjectIdField\n",
      "    filepath:     fiftyone.core.fields.StringField\n",
      "    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "    phash:        fiftyone.core.fields.StringField\n"
     ]
    }
   ],
   "source": [
    "if name in fo.list_datasets():\n",
    "    ds = fo.load_dataset(name)\n",
    "\n",
    "    # Delete any predictions still attached\n",
    "    ds.delete_sample_field(\"predictions\", 2)\n",
    "    ds.delete_evaluations()\n",
    "else:\n",
    "    ds = fo.Dataset(name=name, persistent=True)\n",
    "    for split in splits:\n",
    "        ds.add_dir(\n",
    "            dataset_dir=dataset_dir,\n",
    "            dataset_type=fo.types.YOLOv5Dataset,\n",
    "            include_all_data=True,\n",
    "            split=split,\n",
    "            tags=split,\n",
    "        )\n",
    "\n",
    "    # Add Perceptual Hashes for Dupe Detection later\n",
    "    # Due to multiple false positives, perceptual hash chosen is closer to cryptographic\n",
    "    for sample in ds.iter_samples(progress=True, autosave=True):\n",
    "        sample[\"phash\"] = str(imagehash.dhash(Image.open(sample.filepath)))\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relabel all detections to plushie\n",
    "view = ds.set_field(\n",
    "    \"ground_truth.detections\", F(\"detections\").map(F().set_field(\"label\", \"plushie\"))\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dupe Detection\n",
    "\n",
    "There were no dupes; The Object Detection this time is just that easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter view to only show images where `phash` has more than 1 instance\n",
    "counts = filter(lambda i: i[1] > 1, view.count_values(\"phash\").items())\n",
    "counts = [k for k, v in counts]\n",
    "dupes = view.filter_field(\"phash\", F().is_in(counts)).filter_field(\n",
    "    \"tags\", F().contains([\"train\", \"val\"])\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export YOLO Dataset for Ultralytics\n",
    "\n",
    "Converts `til23plush` to `til23plushonly`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://docs.voxel51.com/api/fiftyone.utils.yolo.html#fiftyone.utils.yolo.YOLOv5DatasetExporter\n",
    "splits = \"train\", \"val\"\n",
    "config = dict(\n",
    "    export_dir=\"data/til23plushonly\",\n",
    "    dataset_type=fo.types.YOLOv5Dataset,\n",
    "    label_field=\"ground_truth\",\n",
    "    export_media=\"symlink\",\n",
    "    include_path=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'data/til23plushonly' already exists; export will be merged with existing files\n",
      " 100% |███████████████| 5664/5664 [5.1s elapsed, 0s remaining, 1.1K samples/s]       \n",
      "Directory 'data/til23plushonly' already exists; export will be merged with existing files\n",
      " 100% |█████████████████| 800/800 [691.1ms elapsed, 0s remaining, 1.2K samples/s]       \n"
     ]
    }
   ],
   "source": [
    "for split in splits:\n",
    "    v = view.filter_field(\"tags\", F().contains([split]))\n",
    "    v.export(split=split, **config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"test\"\n",
    "label_dir = f\"runs/detect/predict/{split}/labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample field 'eval_tp' does not exist\n",
      "Sample field 'eval_fp' does not exist\n",
      "Sample field 'eval_fn' does not exist\n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1600/1600 [3.7s elapsed, 0s remaining, 461.7 samples/s]      \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     plushie       0.00      0.00      0.00       0.0\n",
      "\n",
      "   micro avg       0.00      0.00      0.00       0.0\n",
      "   macro avg       0.00      0.00      0.00       0.0\n",
      "weighted avg       0.00      0.00      0.00       0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sview = view.filter_field(\"tags\", F().contains([split]))\n",
    "fouy.add_yolo_labels(\n",
    "    sample_collection=sview,\n",
    "    label_field=\"predictions\",\n",
    "    labels_path=label_dir,\n",
    "    classes=[\"plushie\"],\n",
    ")\n",
    "results = sview.evaluate_detections(\n",
    "    \"predictions\",\n",
    "    gt_field=\"ground_truth\",\n",
    "    eval_key=\"eval\",\n",
    ")\n",
    "results.print_report()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Image Classification Dataset for Suspect Recognition\n",
    "\n",
    "Converts `til23plush` to `til23reid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = 0.5, 0.0\n",
    "label_field = \"ground_truth\"\n",
    "export_dir = \"data/til23reid\"\n",
    "splits = \"train\", \"val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected an image classification exporter and a label field 'ground_truth' of type <class 'fiftyone.core.labels.Detections'>. Exporting image patches...\n",
      " 100% |█████████████| 12321/12321 [2.1m elapsed, 0s remaining, 95.2 samples/s]       \n",
      "Detected an image classification exporter and a label field 'ground_truth' of type <class 'fiftyone.core.labels.Detections'>. Exporting image patches...\n",
      " 100% |███████████████| 1699/1699 [14.6s elapsed, 0s remaining, 117.3 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "for split, p in zip(splits, padding):\n",
    "    ds.filter_field(\"tags\", F().contains([split])).export(\n",
    "        export_dir=f\"{export_dir}/{split}\",\n",
    "        dataset_type=fo.types.ImageClassificationDirectoryTree,\n",
    "        label_field=label_field,\n",
    "        alpha=p,\n",
    "        image_format=\".png\",\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval Suspect Recognition\n",
    "\n",
    "Used to find optimal threshold parameters.\n",
    "\n",
    "NOTE: Suspect folder used below was handpicked from the `til23reid` dataset.\n",
    "\n",
    "NOTE: We uses suspect images as the detection pool. But 10 > ~3 detections per test image. This might affect parameter tuning accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dir = \"data/til23reid/val\"\n",
    "sus_dir = \"data/val_suspects\"\n",
    "reid_path = \"reid.torchscript\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 1699/1699 [565.6ms elapsed, 0s remaining, 3.0K samples/s]      \n"
     ]
    }
   ],
   "source": [
    "imds = fo.Dataset.from_dir(\n",
    "    dataset_dir=val_dir,\n",
    "    dataset_type=fo.types.ImageClassificationDirectoryTree,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load encoder and encode suspects.\n",
    "encoder = ReIDEncoder(reid_path)\n",
    "embeds = {}\n",
    "for pth in Path(sus_dir).glob(\"*.png\"):\n",
    "    lbl = pth.stem\n",
    "    im = np.array(Image.open(pth))\n",
    "    embeds[lbl] = encoder([im])[0]\n",
    "embeds = sorted(embeds.items())\n",
    "embeds = list(zip(*embeds))\n",
    "sus_cls = list(embeds[0]) # type: ignore\n",
    "sus_embeds = list(embeds[1]) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 1699/1699 [1.2m elapsed, 0s remaining, 23.2 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "# Encode all images.\n",
    "for sample in imds.iter_samples(progress=True, autosave=True):\n",
    "    im = np.array(Image.open(sample.filepath))\n",
    "    embeds = encoder([im])[0]\n",
    "    sample[\"logits\"] = [cos_sim(v, embeds) for v in sus_embeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 1699/1699 [1.5s elapsed, 0s remaining, 1.3K samples/s]         \n"
     ]
    }
   ],
   "source": [
    "imds.delete_sample_fields([\"predictions\", \"bin_ground_truth\", \"bin_predictions\"], 2)\n",
    "for sample in imds.iter_samples(progress=True, autosave=True):\n",
    "    logits = sample[\"logits\"]\n",
    "    gt = sample[\"ground_truth\"].label\n",
    "    \n",
    "    # Multiclass for confusion matrix.\n",
    "    sample[\"predictions\"] = fo.Classification(\n",
    "        label=sus_cls[np.argmax(logits)],\n",
    "        confidence=np.max(logits),\n",
    "        logits=logits,\n",
    "    )\n",
    "\n",
    "    # Binary for PR curve.\n",
    "    # Goal is to find minimum confidence where all false matches disappear.\n",
    "    # NOTE: Due to issue with fiftyone, we select a random label.\n",
    "    # placeholder = np.random.rand() < 0.5\n",
    "    # idx = thres(logits)\n",
    "    # correct = sus_cls[idx] == gt if idx != -1 else False\n",
    "    # conf = np.max(logits)\n",
    "    # conf = 1.0 - conf if idx == -1 else conf\n",
    "    # sample[\"bin_ground_truth\"] = fo.Classification(\n",
    "    #     label=\"N\" if placeholder else \"P\"\n",
    "    # )\n",
    "    # placeholder = placeholder if correct else not placeholder\n",
    "    # sample[\"bin_predictions\"] = fo.Classification(\n",
    "    #     label=\"N\" if placeholder else \"P\",\n",
    "    #     confidence=conf,\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_threshold_function(func, x_axis, suspect_dropout, ds=imds):\n",
    "    np.random.seed(42)\n",
    "    acc_axis = []\n",
    "    p_axis = []\n",
    "    r_axis = []\n",
    "    f_axis = []\n",
    "    all_logits = [np.array(s[\"logits\"]).copy() for s in ds]\n",
    "    all_gts = [sus_cls.index(s[\"ground_truth\"].label) for s in ds]\n",
    "\n",
    "    # NOTE: Seems there is a variance of +- 0.01 for the scores. Oh well.\n",
    "    for thres in tqdm(x_axis):\n",
    "        tp, fp, tn, fn = 0, 0, 0, 0\n",
    "        for logits, gt in zip(all_logits, all_gts): # type: ignore\n",
    "            logits = np.array(logits).copy()\n",
    "            no_suspect = np.random.rand() < suspect_dropout\n",
    "\n",
    "            if no_suspect:\n",
    "                logits[gt] = np.delete(logits, gt).mean()\n",
    "\n",
    "            pred = func(logits, thres)\n",
    "            if no_suspect and pred == -1:\n",
    "                tn += 1\n",
    "            elif not no_suspect and pred == gt:\n",
    "                tp += 1\n",
    "            elif no_suspect and pred != -1:\n",
    "                fp += 1\n",
    "            elif not no_suspect and pred == -1:\n",
    "                fn += 1\n",
    "            elif not no_suspect and pred != gt:\n",
    "                # We don't count false predictions for now.\n",
    "                # fp += 1\n",
    "                pass\n",
    "        \n",
    "        acc = (tp + tn) / max(tp + tn + fp + fn, 1)\n",
    "        p = tp / max(tp + fp, 1)\n",
    "        r = tp / max(tp + fn, 1)\n",
    "        f = 2 * p * r / max(p + r, 1e-6)\n",
    "\n",
    "        acc_axis.append(acc)\n",
    "        p_axis.append(p)\n",
    "        r_axis.append(r)\n",
    "        f_axis.append(f)\n",
    "    \n",
    "    return acc_axis, p_axis, r_axis, f_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 59.29it/s]\n",
      "100%|██████████| 100/100 [00:04<00:00, 22.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Compare PR curves for different threshold functions and tune other threshold params.\n",
    "suspect_dropout = 0.5 # Adversarial case where many images have no suspect.\n",
    "x_axis = np.arange(0.0, 1.0, 0.01)\n",
    "\n",
    "cos_func = lambda logits, thres: np.argmax(logits) if np.max(logits) > thres else -1\n",
    "# Can also vary `vote_thres` and 10*`sd_thres` instead of `accept_thres`.\n",
    "# NOTE: Below are optimal (based on val set) after tuning.\n",
    "a_func = lambda logits, thres: thres_strategy_A(logits, thres, 0.37, 4.4)\n",
    "\n",
    "cos_res = evaluate_threshold_function(cos_func, x_axis, suspect_dropout)\n",
    "a_res = evaluate_threshold_function(a_func, x_axis, suspect_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "# Plot comparison.\n",
    "fig, ax = plt.subplots()\n",
    "# ax.plot(x_axis, cos_res[1], label=\"Cos Precision\")\n",
    "# ax.plot(x_axis, cos_res[2], label=\"Cos Recall\")\n",
    "# ax.plot(x_axis, cos_res[0], label=\"Cos Accuracy\")\n",
    "ax.plot(x_axis, cos_res[3], label=\"Cos F1\")\n",
    "# ax.plot(x_axis, a_res[1], label=\"A Precision\")\n",
    "# ax.plot(x_axis, a_res[2], label=\"A Recall\")\n",
    "# ax.plot(x_axis, a_res[0], label=\"A Accuracy\")\n",
    "ax.plot(x_axis, a_res[3], label=\"A F1\")\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Threshold vs Score\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "print(\"Max Cos F1 & Threshold:\", max(cos_res[3]), x_axis[np.argmax(cos_res[3])])\n",
    "print(\"Max A F1 & Threshold:\", max(a_res[3]), x_axis[np.argmax(a_res[3])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          #0       1.00      1.00      1.00       179\n",
      "          #1       1.00      1.00      1.00       177\n",
      "          #2       1.00      1.00      1.00       169\n",
      "          #3       1.00      0.99      1.00       163\n",
      "          #4       0.98      1.00      0.99       163\n",
      "          #5       0.99      0.98      0.98       163\n",
      "          #6       0.95      0.92      0.93       166\n",
      "          #7       1.00      0.98      0.99       169\n",
      "          #8       0.97      0.99      0.98       173\n",
      "          #9       0.90      0.92      0.91       177\n",
      "\n",
      "    accuracy                           0.98      1699\n",
      "   macro avg       0.98      0.98      0.98      1699\n",
      "weighted avg       0.98      0.98      0.98      1699\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fo.config.requirement_error_level = 1\n",
    "imds.delete_evaluations()\n",
    "results = imds.evaluate_classifications(\n",
    "    \"predictions\",\n",
    "    gt_field=\"ground_truth\",\n",
    "    eval_key=\"eval\",\n",
    ")\n",
    "results.print_report()\n",
    "# display(results.plot_confusion_matrix())\n",
    "# results = imds.evaluate_classifications(\n",
    "#     \"bin_predictions\",\n",
    "#     gt_field=\"bin_ground_truth\",\n",
    "#     eval_key=\"eval_bin\",\n",
    "#     method=\"binary\",\n",
    "#     classes=[\"N\", \"P\"],\n",
    "# )\n",
    "# results.print_report()\n",
    "# display(results.plot_pr_curve())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put whatever view or dataset below\n",
    "# v = ds\n",
    "# v = view\n",
    "# v = dupes\n",
    "# v = sview\n",
    "v = imds\n",
    "fo.launch_app(dataset=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "til",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
