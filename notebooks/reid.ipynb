{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jh/code/til/til-23-cv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "%cd .\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Suspect Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "from til_23_cv import LitImEncoder, ArcMarginProduct, LitImClsDataModule\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    DeviceStatsMonitor,\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings\n",
    "\n",
    "Picking the model backbone, batch size, image size, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['resmlp_12_224.fb_dino',\n",
       " 'resmlp_24_224.fb_dino',\n",
       " 'vit_base_patch8_224.dino',\n",
       " 'vit_base_patch14_dinov2.lvd142m',\n",
       " 'vit_base_patch16_224.dino',\n",
       " 'vit_giant_patch14_dinov2.lvd142m',\n",
       " 'vit_large_patch14_dinov2.lvd142m',\n",
       " 'vit_small_patch8_224.dino',\n",
       " 'vit_small_patch14_dinov2.lvd142m',\n",
       " 'vit_small_patch16_224.dino']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NOTE:\n",
    "# {model}_{size}_{patch size}_{im size}.{train method}_{dataset}\n",
    "# m38m is Merged-38M, combines A LOT of datasets.\n",
    "# ft refers to fine-tuning on a smaller dataset later.\n",
    "# so m38m_ft_in22k_in1k means pretrained on Merged-38M, then finetuned on\n",
    "# ImageNet-22k followed by ImageNet-1k.\n",
    "# Clip models might be useful for their zero-shot capabilities.\n",
    "display(timm.list_models(pretrained=True, filter=\"*dino*\"))\n",
    "# backbone = \"eva02_large_patch14_448.mim_m38m_ft_in22k_in1k\"\n",
    "# backbone = \"eva02_tiny_patch14_336.mim_in22k_ft_in1k\"\n",
    "# https://huggingface.co/timm/vit_small_patch14_dinov2.lvd142m\n",
    "backbone = \"vit_small_patch14_dinov2.lvd142m\"\n",
    "\n",
    "im_size = 224\n",
    "batch_size = 144\n",
    "num_workers = 12"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "data = LitImClsDataModule(\n",
    "    \"data/til23reid\",\n",
    "    im_size=im_size,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "# `setup()` is automatically called; Calling here to check number of classes.\n",
    "data.setup(\"fit\")\n",
    "nclasses = len(data.train_ds.classes)\n",
    "print(nclasses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "while False:\n",
    "    im = data.preview_transform(1)[0]\n",
    "    im = im.resize((1024, 1024))\n",
    "    im = np.array(im)[...,::-1]\n",
    "    cv2.imshow(\"example\", im)\n",
    "    key = chr(cv2.waitKey(0))\n",
    "    if key == \"q\":\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_pretrain.pth',\n",
       " 'hf_hub_id': 'timm/vit_small_patch14_dinov2.lvd142m',\n",
       " 'architecture': 'vit_small_patch14_dinov2',\n",
       " 'tag': 'lvd142m',\n",
       " 'custom_load': False,\n",
       " 'input_size': (3, 518, 518),\n",
       " 'fixed_input_size': True,\n",
       " 'interpolation': 'bicubic',\n",
       " 'crop_pct': 1.0,\n",
       " 'crop_mode': 'center',\n",
       " 'mean': (0.485, 0.456, 0.406),\n",
       " 'std': (0.229, 0.224, 0.225),\n",
       " 'num_classes': 0,\n",
       " 'pool_size': None,\n",
       " 'first_conv': 'patch_embed.proj',\n",
       " 'classifier': 'head',\n",
       " 'license': 'cc-by-nc-4.0'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LitImEncoder(\n",
    "    backbone,\n",
    "    head=ArcMarginProduct(384, nclasses, s=32, m=0.5),\n",
    "    pretrained=True,\n",
    "    nclasses=nclasses,\n",
    "    im_size=im_size,\n",
    ")\n",
    "# Set corresponding dataloader image normalization configs.\n",
    "display(model.model.pretrained_cfg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lit Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html\n",
    "callbacks = [\n",
    "    # DeviceStatsMonitor(),\n",
    "    LearningRateMonitor(logging_interval=\"step\"),\n",
    "    EarlyStopping(monitor=\"val_sil_score\", patience=20, mode=\"max\", strict=True),\n",
    "    ModelCheckpoint(\n",
    "        filename=\"{epoch}-{val_sil_score:.2f}\",\n",
    "        monitor=\"val_sil_score\",\n",
    "        save_last=True,\n",
    "        save_top_k=3,\n",
    "        mode=\"max\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "# torch.set_float32_matmul_precision(\"high\")\n",
    "seed_everything(42, workers=True)\n",
    "trainer = Trainer(\n",
    "    # TODO: This is the part that would love Hydra config system.\n",
    "    # default_root_dir=\"temp\",\n",
    "    benchmark=True,\n",
    "    # Glitchy, see https://github.com/Lightning-AI/lightning/issues/5558.\n",
    "    # precision=\"16-mixed\",\n",
    "    callbacks=callbacks,\n",
    "    log_every_n_steps=10,\n",
    "    check_val_every_n_epoch=1, # 3\n",
    "    max_steps=10000,\n",
    "    # detect_anomaly=True,\n",
    "    # fast_dev_run=True, # Debugging.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, datamodule=data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"epoch=8-val_sil_score=0.43.pt\"\n",
    "save_path = \"reid.torchscript\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load checkpoint.\n",
    "ckpt = torch.load(ckpt_path)\n",
    "model.load_state_dict(ckpt[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jh/.conda/envs/til/lib/python3.9/site-packages/torch/__init__.py:1209: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert condition, message\n"
     ]
    }
   ],
   "source": [
    "# Trace & save model.\n",
    "traced = torch.jit.trace(model.model, torch.rand(1, 3, im_size, im_size))\n",
    "torch.jit.save(traced, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# Check equality.\n",
    "traced = torch.jit.load(save_path)\n",
    "x = torch.rand(1, 3, im_size, im_size)\n",
    "print(torch.isclose(traced(x), model.model(x)).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "til",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
